%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{tsml.cara.rct}
%\VignetteDepends{tsml.cara.rct}
%\VignetteKeywords{targeted  sequential  design for  targeted  data-adaptive inference of the optimal treatment rule and its mean reward}
%\VignettePackage{tsml.cara.rct}

\documentclass{article}
\usepackage{amsmath,amssymb,manfnt}
\graphicspath{{fig/}}

<<setup, include=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='fig/'
)
@

<<style:knitr, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@


\addtolength{\parskip}{\baselineskip}


\newcommand{\1}{{\bf 1}}
\DeclareMathOperator{\Var}{Var}
\newcommand{\bQ}{\bar{Q}}
\newcommand{\gb}{g^{\rm b}}
\newcommand{\bfg}{{\bf g}}


\title{Using the \Githubpkg{achambaz/tsml.cara.rct} package}

\author{Antoine Chambaz\\\email{achambaz@u-paris10.fr}}

\date{Package version \Sexpr{packageDescription("tsml.cara.rct")$Version}\\Date \Sexpr{packageDescription("tsml.cara.rct")$Date}}


\begin{document}

\maketitle 
\tableofcontents 

% The caution symbol \textdbend marks important details.



\section{Citing \Rpackage{tsml.cara.rct}}

If  you  use  the   \Githubpkg{achambaz/tsml.cara.rct}  package,  please  cite
\cite{chambaz15,zheng16} and/or \cite{chambaz16}.

\section{Using  the \Rpackage{tsml.cara.rct} \texttt{R} package} 

The \Githubpkg{achambaz/tsml.cara.rct} package is meant to simulate easily the
sampling from  a targeted sequential  design and the  targeted (data-adaptive)
inference  of the  Average  Treatment  Effect (ATE)  or  Mean  of the  Optimal
treatment   Rule   (MOR)    based   on   accrued   data    as   described   in
\cite{chambaz15,zheng16,chambaz16}.  We assume that  the user is familiar with
the procedures.

The $n$th observation is $O_{n}  \equiv (W_{n},A_{n},Y_{n})$ where $W_{n}$ are
the  $n$th baseline  covariates, $A_{n}  \in  \{0,1\}$ is  the $n$th  assigned
treatment  and  $Y_{n}$  is  the $n$th  outcome/reward.   The  data-generating
distribution  of  $O_{1},   \ldots,  O_{n}$  is  identified   by  $Q_{0}$  and
$\bfg_{n} \equiv  (g_{1}, \ldots, g_{n})$.   The former identifies  the common
marginal  distribution  of the  independent  $W_{1},  \ldots, W_{n}$  and  the
conditional distribution of  each $Y_{i}$ given $(A_{i},  W_{i})$.  The latter
identifies the conditional distributions of all $A_{i}$ given $W_{i}$, $A_{i}$
being drawn  from the Bernoulli distribution  with parameter $g_{i}(1|W_{i})$,
where    the    data-adaptive    definition     of    $g_{i}$    depends    on
$O_{1}, \ldots, O_{i-1}$.

The function \Rfunction{getSample} simulates independent observations from the
distribution $P_{Q_{0}, g}$ such that $O  \equiv (W,A,Y) \sim P_{Q_{0}, g}$ is
sequentially drawn as follows:
\begin{itemize}
\item  $W$  equals  $(U,V)$  with  $U$  and  $V$  independent,  $U$  uniformly
  distributed over $[0,1]$ and $V \in  \{1, \ldots, \nu\}$ with a distribution
  characterized  by argument  \Robject{piV}, a  vector of  length $\nu$  whose
  $v$th component equals the probability of $V=v$;
\item $A$ is  drawn conditionally on $W$ from the  Bernoulli distribution with
  parameter  $g(1|W)$, characterized  by  argument  \Robject{tm}, a  treatment
  mechanism;
\item $Y$  is drawn  conditionally on  $(A,W)$ from either  the Gamma  or Beta
  distribution, depending on argument  \Robject{family}, with conditional mean
  and variance characterized by arguments \Robject{Qbar} and \Robject{Vbar}.
\end{itemize}

If argument \Robject{what} is not missing and equals either \Robject{"ATE"} or
\Robject{"MOR"},  then \Rfunction{getSample}  also allows  the computation  of
(Monte Carlo approximations  to) the true values of the  parameter of interest
and standard deviation  of the efficient influence curve  of the corresponding
mapping  evaluated   at  $P_{Q_{0},   g}$.   If   \Robject{what}  is   set  to
\Robject{"MOR"}, then (a Monte Carlo  approximation to) the standard deviation
of the  efficient influence  curve of the  corresponding mapping  evaluated at
$P_{Q_{0},  r_{0}}$ is  also computed,  where $r_{0}  \equiv r(Q_{0})$  is the
optimal treatment rule under $Q_{0}$.

\subsection{Set up}

First, we set the verbosity parameter and random seed.

<<seed, message=FALSE>>=
library("R.utils")
library("tsml.cara.rct")
log <- Arguments$getVerbose(-8, timestamp=TRUE)
set.seed(54321)
@

\subsection{Average Treatment Effect}

Here,  we  choose  \Robject{piV=c(1/2, 1/3,  1/6)},  \Robject{family="gamma"},
\Robject{Qbar=Qbar1}, 
\begin{equation*}
  \bQ_{Y,0} (A,W) \equiv E_{Q_{0}} (Y|A,W) =  A V + \frac{1-A}{1+V} + 2U^{2} +
  2U +1, 
\end{equation*}
\Robject{Vbar=Vbar1}, 
\begin{equation*}
  \Var_{Q_{0}} (Y|A,W) = A (1+U+V)^{2} + (1-A)\left(\frac{1}{1+V}+U\right)^{2}, 
\end{equation*}
target the ATE
\begin{equation*}
  \psi_{0}^{\rm ATE}  \equiv E_{Q_{0}} \left(\bQ_{Y,0} (1,  W) - \bQ_{Y,0}
    (0, W)\right) = \frac{91}{72}, 
\end{equation*}
and aim  at minimizing the  asymptotic variance  of our targeted  minimum loss
estimator (TMLE)  $\psi_{n}^{*}$.  The  minimization is relative  to $g$  in a
working model chosen by us.

When the  minimization is relative  to $g$ in  the working model  described by
\Rcode{tm.model=formula(A~1)}, which  assigns treatment regarless of  $W$, the
smallest asymptotic standard  deviation that our estimator can  achieve can be
computed     approximately    as     follows    (based     on    \Rcode{n=1e5}
i.i.d. observations):
<<psi:sd:ATE>>=
psi.sd <- sqrt(getOptVar(n=1e5,
                         tm.model=formula(A~1),
                         piV=c(1/2, 1/3, 1/6),
                         family="gamma",
                         Qbar=Qbar1,
                         Vbar=Vbar1))
truth <- c(psi=91/72, psi.sd=psi.sd)
truth
@ 

\subsubsection{Simulation using \Rfunction{tsml.cara.rct}}

Let us  consider the case that  we rely on  the (fixed) working model  for $g$
described  by  \Rcode{tm.model}  and  (fixed) working  model  for  $\bQ_{Y,0}$
described by \Rcode{learnQ} as defined below:

<<tm:model:ATE, eval=TRUE>>=
tm.model <- formula(A~.)
learnQ <- formula(Y~I(as.integer(A)):(U+V)+I(as.integer(1-A)):(U+V))
@ 

The  \Rcode{ninit=200}   first  observations  are  drawn   independently  from
$P_{Q_{0}, \gb}$,  where $\gb$ is  the balanced treatment  mechanism, function
\Rfunction{oneOne}. Then, the design is updated  every time the sample size is
a  multiple of  \Rcode{by=100}, using  \Rcode{tm.ref=oneOne} as  the reference
treatment mechanism.

<<ATE:param, warning=FALSE, eval=TRUE>>=
## ATE, parametric example
ATE.param <- tsml.cara.rct(what="ATE",
                           flavor="parametric",
                           ninit=200,
                           by=100,
                           nmax=500,
                           tm.init=oneOne,
                           tm.ref=oneOne,
                           learnQ=learnQ,
                           tm.model=tm.model,
                           conf.level=0.95,
                           piV=c(1/2, 1/3, 1/6),
                           family="gamma",
                           Qbar=Qbar1,
                           Vbar=Vbar1)
ATE.param
@ 

A  visual  representation  of  the  course   of  the  procedure  is  shown  in
Figure~\ref{fig/fig:ATE:param-1}. It is obtained by running

<<fig:ATE:param, fig.show='hide', fig.width=6, fig.height=6, eval=TRUE>>=
plot(ATE.param, truth=truth)
@

\incfig{fig/fig:ATE:param-1}{0.5\textwidth}{The   course   of  the   procedure
  summarized by \Robject{ATE.param}.}{}

We may prefer to estimate $\bQ_{Y,0}$  by fitting a larger working model using
the LASSO.  Redefine \Robject{learnQ} by setting

<<learnQ, eval=TRUE>>= 
coeffs <- attr(poly(seq(0, 1, 1e-3), 3), "coefs")
learnQ <- formula(Y~I(as.integer(A)):(poly(U, 3, coefs=coeffs)*V)+
                    I(as.integer(1-A)):(poly(U, 3, coefs=coeffs)*V))
@ 

and running

<<ATE:lasso, warning=FALSE, eval=TRUE>>=
## ATE, lasso example
ATE.lasso <- tsml.cara.rct(what="ATE",
                           flavor="lasso",
                           ninit=200,
                           by=100,
                           nmax=500,
                           learnQ=learnQ,
                           tm.init=oneOne,
                           tm.ref=oneOne,
                           tm.model=tm.model,
                           conf.level=0.95,
                           piV=c(1/2, 1/3, 1/6),
                           family="gamma",
                           Qbar=Qbar1,
                           Vbar=Vbar1)
ATE.lasso
@ 


\subsubsection{Step by step simulation}

It is also  possible to carry out  the procedure step by  step.  Introduce the
working model for $\bQ_{Y,0}$ described by \Rcode{learnQ} as redefined below:

<<learnQ:bis>>=
learnQ <- formula(Y~A*V)
@ 



<<ATE:param:step, warning=FALSE, eval=TRUE>>=
## ATE, another parametric example, step by step
obs <- getSample(200, 
                 piV=c(1/2, 1/3, 1/6),
                 family="gamma",
                 Qbar=Qbar1,
                 Vbar=Vbar1)
nobs <- nrow(obs)
ATE.param.step <- tsml.cara.rct:::TSMLCARA(what="ATE",
                                           obs=obs,
                                           learnQ=learnQ,
                                           tm.model=tm.model,
                                           Gmin=1e-2,
                                           flavor="parametric",
                                           verbose=log)
while(nobs<500) {
  tsml.cara.rct:::update.TSMLCARA(ATE.param.step, verbose=log)
  tsml.cara.rct:::targetPsi.TSMLCARA(ATE.param.step, verbose=log)
  gstar <- tsml.cara.rct:::getGstar.TSMLCARA(ATE.param.step)
  newObs <- getSample(100, tm=gstar,
                      piV=c(1/2, 1/3, 1/6),
                      family="gamma",
                      Qbar=Qbar1,
                      Vbar=Vbar1)
  tsml.cara.rct:::addNewSample.TSMLCARA(ATE.param.step, newObs)
  nobs <- nrow(getObs(ATE.param.step))
}
tsml.cara.rct:::update.TSMLCARA(ATE.param.step, verbose=log)
tsml.cara.rct:::targetPsi.TSMLCARA(ATE.param.step, verbose=log)
ATE.param.step
@ 



\subsection{Mean of the Optimal treatment Rule}

Here,  we  choose  \Robject{piV=c(1/2, 1/3,  1/6)},  \Robject{family="beta"},
\Robject{Qbar=Qbar2}, 
\begin{equation*}
  \bQ_{Y,0}   (A,W)    =  A   \left(\frac{1}{2}  +
    \frac{3}{8}\cos(\pi UV) \right) + (1-A) \left(\frac{1}{2} +
    \frac{1}{4}\sin(3\pi U/V)\right), 
\end{equation*}
\Robject{Vbar=Vbar2} such that $\Var_{Q_{0}} (Y|A,W)=0.1$, 
and target the MOR
\begin{equation*}
  \psi_{0}^{\rm MOR} \equiv E_{Q_{0}} \left(\bQ_{Y,0} (r_{0}(W), W)\right) 
\end{equation*}
where the optimal treatment rule $r_{0} \equiv r(\bQ_{Y,0})$ is given by
\begin{equation*}
  r_{0} (W) \equiv \1\{\bQ_{Y,0}(1,W) \geq \bQ_{Y,0}(0,W)\}.
\end{equation*}

The true  parameter value, optimal  standard deviations under $\gb$  and under
$r_{0}$ can be computed approximately as follows:

<<truth:MOR>>=
truth <- getSample(1e5,
                   tm=oneOne,
                   rule=NULL,
                   piV=c(1/2, 1/3, 1/6),
                   family="beta",
                   Qbar=Qbar2,
                   Vbar=Vbar2,
                   what="MOR")
truth
@ 



Say we choose to estimate $\bQ_{Y,0}$ by fitting larger, sample-size-dependent
working models using the  LASSO.  Function \Rfunction{makeLearnQ} creates such
working     models.     It     determines     two    fine-tune     parameters,
\Rcode{deg=3+floor(n/500)} and  \Rcode{nlev=ceiling(n/250)}, where \Robject{n}
is  the sample  size.   When \Robject{nlev}  equals 1,  the  working model  is
described by a \Robject{formula} similar to
<<formula:nlev:one, eval=FALSE>>=
formula(Y~I(A=0)*V*poly(U, deg)+I(A=1)*V*poly(U, deg))
@   

Otherwise, the working model is described by a more complex \Robject{formula},
summarized by the following {\em pseudo}-\Robject{formula}:
<<formula:nlev:two, eval=FALSE>>=
formula(Y~I(A=0)*V*(poly(U, deg)+step(U))+
          I(A=1)*V*poly(U, deg)+step(U))
@   

where  \Robject{step(U)}  consits  of \Robject{nlev}  indicator  functions  of
subsets of  $[0,1]$ of the form  $\{x: x<k/\Robject{nlev}\}$. We also  let the
derivation of the treatment rules used  to assign treatment from the estimates
of   the  blip   function   depend   on  sample   size,   by  specifying   the
\Robject{Gexploit} and \Robject{Gexplore} arguments.

<<MOR:lasso, warning=FALSE, eval=TRUE>>=
## MOR, lasso example
Gexploit <- function(nn) {
  if (nn <= 300) {
    exploit <- 0.15
  } else if (nn <= 400) {
    exploit <- 0.10
  } else if (nn <= 500) {
    exploit <- 0.05
  } else {
    exploit <- 0.025
  }
  return(exploit)
}
Gexplore <- Gexploit

MOR.lasso <- tsml.cara.rct(what="MOR",
                           flavor="lasso",
                           ninit=200,
                           by=100,
                           nmax=500,
                           learnQ=makeLearnQ,
                           tm.init=oneOne,
                           tm.ref=oneOne,
                           Gmin=0.05,
                           Gexploit=Gexploit,
                           Gexplore=Gexplore,
                           conf.level=0.95,
                           piV=c(1/2, 1/3, 1/6),
                           family="beta",
                           Qbar=Qbar2,
                           Vbar=Vbar2)                           
MOR.lasso
@ 

A ``history''  of the procedure is  contained in \Rcode{MOR.lasso}. It  can be
retrieved by running

<<MOR:history, eval=TRUE>>=
MOR.lasso.hist <- getHistory(MOR.lasso)$hist
MOR.lasso.hist
@ 

A visual representation of the course  of the procedure, including regrets and
their estimation, is shown in Figure~\ref{fig/fig:MOR:lasso-1}. It is obtained
by running

<<fig:MOR:lasso, fig.show='hide', fig.width=6, fig.height=6, eval=TRUE>>=
plot(MOR.lasso, 
     truth=list(truth, attr(MOR.lasso, "truthn")),
     regret=list(attr(MOR.lasso, "eregretn"), attr(MOR.lasso, "cregretn")))
@

\incfig{fig/fig:MOR:lasso-1}{0.5\textwidth}{The   course   of  the   procedure
  summarized by \Robject{MOR.lasso}.}{}

It is  intereting to compare  the above  procedure with the  similar procedure
which differs  from it  only by  the use of  the balanced  treatment mechanism
throughout (\Rcode{Gmin=0.5}). 

<<MOR.balanced, warning=FALSE, eval=TRUE>>=
MOR.balanced <- tsml.cara.rct(what="MOR",
                              flavor="lasso",
                              ninit=200,
                              by=100,
                              nmax=500,
                              learnQ=makeLearnQ,
                              tm.init=oneOne,
                              tm.ref=oneOne,
                              Gmin=0.5,
                              conf.level=0.95,
                              piV=c(1/2, 1/3, 1/6),
                              family="beta",
                              Qbar=Qbar2,
                              Vbar=Vbar2)                           
MOR.balanced
MOR.balanced.hist <- getHistory(MOR.balanced)$hist

## comparing variances and regrets
idx <- grep("target", rownames(MOR.balanced.hist))
nobs <- MOR.balanced.hist[idx, "nobs"]
var.ratio <- (MOR.balanced.hist[idx, "psi.sd"]/MOR.lasso.hist[idx, "psi.sd"])^2
regret.ratio <- MOR.balanced.hist[idx, "regret"]/MOR.lasso.hist[idx, "regret"]
cbind(nobs=nobs, var.ratio=var.ratio, regret.ratio=regret.ratio)
@ 




\section{Extensions}

The       functions       \Rfunction{getSample},       \Rfunction{makeLearnQ},
\Rfunction{makeLearnQ.piecewise} and more generally all exported functions can
serve as templates to extend the functionalities of the package.

\section{Session information}
\label{sec:session-information}

Here is  the output  of \Rfunction{sessionInfo}  on the  system on  which this
document was compiled. 

<<sessionInfo>>=
sessionInfo()
@


\begin{thebibliography}{}
\bibitem[Chambaz et~al., 2015]{chambaz15} 
  Chambaz,  A., van~der Laan, M.~J.,  and    Zheng,   W.    (2015).     
  \newblock   Targeted    covariate-adjusted   response-adaptive   LASSO-based
  randomized controlled trials
  \newblock book chapter  in {\em Modern Adaptive  Randomized Clinical Trials:
    Statistical, Operational, and Regulatory Aspects} by A. Sverdlov
  \newblock CRC Press.

\bibitem[Zheng et~al., 2016]{zheng16} 
  Zheng, W., Chambaz, A., and van~der Laan, M.~J. (2016).  
  \newblock   Drawing  valid   targeted   inference  when   covariate-adjusted
  response-adaptive  RCT meets  data-adaptive loss-based  estimation, with  an
  application to the LASSO 
  \newblock {\em Submitted}.  https://hal.archives-ouvertes.fr/hal-01180719
  
\bibitem[Chambaz et~al., 2016]{chambaz16}
  Chambaz, A., Zheng, W., and van~der Laan, M.~J. (2016).
  \newblock Targeted  sequential design for targeted  data-adaptive inference of
  the optimal treatment rule and its mean reward 
  \newblock {\em Submitted}.
\end{thebibliography}



\end{document}




