%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{tsml.cara.rct}
%\VignetteDepends{tsml.cara.rct}
%\VignetteKeywords{targeted  sequential  design for  targeted  data-adaptive inference of the optimal treatment rule and its mean reward}
%\VignettePackage{tsml.cara.rct}

\documentclass{article}
\usepackage{amsmath,amssymb,manfnt}
\graphicspath{{fig/}}

<<setup, include=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='fig/'
)
@

<<style:knitr, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@


\addtolength{\parskip}{\baselineskip}


\newcommand{\1}{{\bf 1}}
\DeclareMathOperator{\Var}{Var}
\newcommand{\bQ}{\bar{Q}}
\newcommand{\gb}{g^{\rm b}}
\newcommand{\bfg}{{\bf g}}


\title{Using the \Githubpkg{achambaz/tsml.cara.rct} package}

\author{Antoine Chambaz\\\email{achambaz@u-paris10.fr}}

\date{Package version \Sexpr{packageDescription("tsml.cara.rct")$Version}\\Date \Sexpr{packageDescription("tsml.cara.rct")$Date}}


\begin{document}

\maketitle 
\tableofcontents 

% The caution symbol \textdbend marks important details.



\section{Citing \Rpackage{tsml.cara.rct}}

If you use  the \Githubpkg{achambaz/tsml.cara.rct} package, please  cite it as
well    as    the    relevant   articles    \cite{chambaz15,zheng16}    and/or
\cite{chambaz16}.       See        \Rcode{citation("tsml.cara.rct")}       and
\Rcode{toBibtex(citation("tsml.cara.rct"))}.

\section{Using  the \Rpackage{tsml.cara.rct} \texttt{R} package} 

The \Githubpkg{achambaz/tsml.cara.rct} package is meant to simulate easily the
sampling from  a targeted sequential  design and the  targeted (data-adaptive)
inference  of the  Average  Treatment  Effect (ATE)  or  Mean  of the  Optimal
treatment   Rule   (MOR)    based   on   accrued   data    as   described   in
\cite{chambaz15,zheng16,chambaz16}.  We assume that  the user is familiar with
the procedures.

The $n$th observation is $O_{n}  \equiv (W_{n},A_{n},Y_{n})$ where $W_{n}$ are
the  $n$th baseline  covariates, $A_{n}  \in  \{0,1\}$ is  the $n$th  assigned
treatment  and  $Y_{n}$  is  the $n$th  outcome/reward.   The  data-generating
distribution  of  $O_{1},   \ldots,  O_{n}$  is  identified   by  $Q_{0}$  and
$\bfg_{n} \equiv  (g_{1}, \ldots, g_{n})$.   The former identifies  the common
marginal  distribution  of the  independent  $W_{1},  \ldots, W_{n}$  and  the
conditional distribution of  each $Y_{i}$ given $(A_{i},  W_{i})$.  The latter
identifies the conditional distributions of all $A_{i}$ given $W_{i}$, $A_{i}$
being drawn  from the Bernoulli distribution  with parameter $g_{i}(1|W_{i})$,
where    the    data-adaptive    definition     of    $g_{i}$    depends    on
$O_{1}, \ldots, O_{i-1}$.

The function \Rfunction{getSample} simulates independent observations from the
distribution $P_{Q_{0}, g}$ such that $O  \equiv (W,A,Y) \sim P_{Q_{0}, g}$ is
sequentially drawn as follows:
\begin{itemize}
\item  $W$  equals  $(U,V)$  with  $U$  and  $V$  independent,  $U$  uniformly
  distributed over $[0,1]$ and $V \in  \{1, \ldots, \nu\}$ with a distribution
  characterized  by argument  \Robject{piV}, a  vector of  length $\nu$  whose
  $v$th component equals the probability of $V=v$;
\item $A$ is  drawn conditionally on $W$ from the  Bernoulli distribution with
  parameter  $g(1|W)$, characterized  by  argument  \Robject{tm}, a  treatment
  mechanism;
\item $Y$  is drawn  conditionally on  $(A,W)$ from either  the Gamma  or Beta
  distribution, depending on argument  \Robject{family}, with conditional mean
  and variance characterized by arguments \Robject{Qbar} and \Robject{Vbar}.
\end{itemize}

If argument \Robject{what} is not missing and equals either \Robject{"ATE"} or
\Robject{"MOR"},  then \Rfunction{getSample}  also allows  the computation  of
(Monte Carlo approximations  to) the true values of the  parameter of interest
and standard deviation  of the efficient influence curve  of the corresponding
mapping  evaluated   at  $P_{Q_{0},   g}$.   If   \Robject{what}  is   set  to
\Robject{"MOR"}, then (a Monte Carlo  approximation to) the standard deviation
of the  efficient influence  curve of the  corresponding mapping  evaluated at
$P_{Q_{0},  r_{0}}$ is  also computed,  where $r_{0}  \equiv r(Q_{0})$  is the
optimal treatment rule under $Q_{0}$.

\subsection{Set up}

First, we set the verbosity parameter and random seed.

<<seed, message=FALSE>>=
library("R.utils")
library("tsml.cara.rct")
log <- Arguments$getVerbose(-8, timestamp=TRUE)
set.seed(54321)
@

\subsection{Average Treatment Effect}

Here,  we  choose  \Robject{piV=c(1/2, 1/3,  1/6)},  \Robject{family="gamma"},
\Robject{Qbar=Qbar1}, 
\begin{equation*}
  \bQ_{Y,0} (A,W) \equiv E_{Q_{0}} (Y|A,W) =  A V + \frac{1-A}{1+V} + 2U^{2} +
  2U +1, 
\end{equation*}
\Robject{Vbar=Vbar1}, 
\begin{equation*}
  \Var_{Q_{0}} (Y|A,W) = A (1+U+V)^{2} + (1-A)\left(\frac{1}{1+V}+U\right)^{2}, 
\end{equation*}
target the ATE
\begin{equation*}
  \psi_{0}^{\rm ATE}  \equiv E_{Q_{0}} \left(\bQ_{Y,0} (1,  W) - \bQ_{Y,0}
    (0, W)\right) = \frac{91}{72}, 
\end{equation*}
and aim  at minimizing the  asymptotic variance  of our targeted  minimum loss
estimator (TMLE)  $\psi_{n}^{*}$.  The  minimization is relative  to $g$  in a
working model chosen by us.

When the  minimization is relative  to $g$ in  the working model  described by
\Rcode{tm.model=formula(A~1)}, which  assigns treatment regarless of  $W$, the
smallest asymptotic standard  deviation that our estimator can  achieve can be
computed     approximately    as     follows    (based     on    \Rcode{n=1e5}
i.i.d. observations):
<<psi:sd:ATE>>=
piV <- c(1/2, 1/3, 1/6)
family <- "gamma"
psi.sd <- sqrt(getOptVar(n=1e5,
                         tm.model=formula(A~1),
                         piV=piV, family=family,
                         Qbar=Qbar1, Vbar=Vbar1))
truth <- c(psi=91/72, psi.sd=psi.sd)
truth
@ 

\subsubsection{Simulation using \Rfunction{tsml.cara.rct}}

Let us  consider the case that  we rely on  the (fixed) working model  for $g$
described  by  \Rcode{tm.model}  and  (fixed) working  model  for  $\bQ_{Y,0}$
described by \Rcode{learnQ} as defined below:

<<tm:model:ATE, eval=TRUE>>=
tm.model <- formula(A~.)
learnQ <- formula(Y~I(as.integer(A)):(U+V)+I(as.integer(1-A)):(U+V))
@ 

The  \Rcode{ninit=200}   first  observations  are  drawn   independently  from
$P_{Q_{0}, \gb}$,  where $\gb$ is  the balanced treatment  mechanism, function
\Rfunction{oneOne}. Then, the design is updated  every time the sample size is
a  multiple of  \Rcode{by=100}, using  \Rcode{tm.ref=oneOne} as  the reference
treatment mechanism.

<<ATE:param, warning=FALSE, eval=TRUE>>=
## ATE, parametric example
ATE.param <- tsml.cara.rct(what="ATE", flavor="parametric",
                           ninit=200, by=100, nmax=500,
                           tm.init=oneOne, tm.ref=oneOne,
                           learnQ=learnQ, tm.model=tm.model,
                           conf.level=0.95,
                           piV=piV, family=family,
                           Qbar=Qbar1, Vbar=Vbar1)
ATE.param
@ 

A  visual  representation  of  the  course   of  the  procedure  is  shown  in
Figure~\ref{fig/fig:ATE:param-1}. It is obtained by running:

<<fig:ATE:param, fig.show='hide', fig.width=6, fig.height=6, eval=TRUE>>=
plot(ATE.param, truth=truth)
@

\incfig{fig/fig:ATE:param-1}{0.5\textwidth}{The   course   of  the   procedure
  summarized by \Robject{ATE.param}.}{}

We may prefer to estimate $\bQ_{Y,0}$  by fitting a larger working model using
the LASSO.  Redefine \Robject{learnQ} by setting

<<learnQ, eval=TRUE>>= 
coeffs <- attr(poly(seq(0, 1, 1e-3), 3), "coefs")
learnQ <- formula(Y~I(as.integer(A)):(poly(U, 3, coefs=coeffs)*V)+
                    I(as.integer(1-A)):(poly(U, 3, coefs=coeffs)*V))
@ 

and run:

<<ATE:lasso, warning=FALSE, eval=TRUE>>=
## ATE, lasso example
ATE.lasso <- tsml.cara.rct(what="ATE", flavor="lasso",
                           ninit=200, by=100, nmax=500,
                           tm.init=oneOne, tm.ref=oneOne,
                           learnQ=learnQ, tm.model=tm.model,
                           conf.level=0.95,
                           piV=piV, family=family,
                           Qbar=Qbar1, Vbar=Vbar1)
ATE.lasso
@ 


\subsubsection{Step by step simulation}

It is also  possible to carry out  the procedure step by  step.  Introduce the
working model for $\bQ_{Y,0}$ described by \Rcode{learnQ} as redefined below:

<<learnQ:bis>>=
learnQ <- formula(Y~A*V)
@ 

Then run:

<<ATE:param:step, warning=FALSE, eval=TRUE>>=
## ATE, another parametric example, step by step
## ## sample
obs <- getSample(200, 
                 piV=piV, family=family,
                 Qbar=Qbar1, Vbar=Vbar1)
nobs <- nrow(obs)
## ## initialize
ATE.param.step <- tsml.cara.rct:::TSMLCARA(obs=obs,
                                           what="ATE", flavor="parametric",
                                           learnQ=learnQ, tm.model=tm.model,
                                           Gmin=0.01, verbose=log)
while(nobs<500) {
  ## ## update, target
  tsml.cara.rct:::update.TSMLCARA(ATE.param.step, verbose=log)
  tsml.cara.rct:::targetPsi.TSMLCARA(ATE.param.step, verbose=log)
  gstar <- tsml.cara.rct:::getGstar.TSMLCARA(ATE.param.step)
  ## ## sample
  newObs <- getSample(100, tm=gstar,
                      piV=piV, family=family,
                      Qbar=Qbar1, Vbar=Vbar1)
  tsml.cara.rct:::addNewSample.TSMLCARA(ATE.param.step, newObs)
  nobs <- nrow(getObs(ATE.param.step))
}
## ## update, target
tsml.cara.rct:::update.TSMLCARA(ATE.param.step, verbose=log)
tsml.cara.rct:::targetPsi.TSMLCARA(ATE.param.step, verbose=log)
ATE.param.step
@ 



\subsection{Mean of the Optimal treatment Rule}

Here,  we  choose  \Robject{piV=c(1/2, 1/3,  1/6)},  \Robject{family="beta"},
\Robject{Qbar=Qbar2}, 
\begin{equation*}
  \bQ_{Y,0}   (A,W)    =  A   \left(\frac{1}{2}  +
    \frac{3}{8}\cos(\pi UV) \right) + (1-A) \left(\frac{1}{2} +
    \frac{1}{4}\sin(3\pi U/V)\right), 
\end{equation*}
\Robject{Vbar=Vbar2} such that $\Var_{Q_{0}} (Y|A,W)=0.01$, 
and target the MOR
\begin{equation*}
  \psi_{0}^{\rm MOR} \equiv E_{Q_{0}} \left(\bQ_{Y,0} (r_{0}(W), W)\right) 
\end{equation*}
where the optimal treatment rule $r_{0} \equiv r(\bQ_{Y,0})$ is given by
\begin{equation*}
  r_{0} (W) \equiv \1\{\bQ_{Y,0}(1,W) \geq \bQ_{Y,0}(0,W)\}.
\end{equation*}

The true  parameter value, optimal  standard deviations under $\gb$  and under
$r_{0}$ can be computed approximately as follows:

<<truth:MOR>>=
family <- "beta"
truth <- getSample(1e5, what="MOR",
                   tm=oneOne, rule=NULL,
                   piV=piV, family=family,
                   Qbar=Qbar2, Vbar=Vbar2)
truth
@ 

\subsubsection{Simulation using \Rfunction{tsml.cara.rct}}

Say we choose to estimate $\bQ_{Y,0}$ by fitting larger, sample-size-dependent
working models using the  LASSO.  Function \Rfunction{makeLearnQ} creates such
working     models.     It     determines     two    fine-tune     parameters,
\Rcode{deg=3+floor(n/500)} and  \Rcode{nlev=ceiling(n/250)}, where \Robject{n}
is  the sample  size.   When \Robject{nlev}  equals 1,  the  working model  is
described by a \Robject{formula} similar to
<<formula:nlev:one, eval=FALSE>>=
formula(Y~I(A=0)*V*poly(U, deg)+I(A=1)*V*poly(U, deg))
@   

Otherwise, the working model is described by a more complex \Robject{formula},
summarized by the following {\em pseudo}-\Robject{formula}:
<<formula:nlev:two, eval=FALSE>>=
formula(Y~I(A=0)*V*(poly(U, deg)+step(U))+
          I(A=1)*V*poly(U, deg)+step(U))
@   

where  \Robject{step(U)}  consits  of \Robject{nlev}  indicator  functions  of
subsets of  $[0,1]$ of the form  $\{x: x<k/\Robject{nlev}\}$. We also  let the
derivation of the treatment rules used  to assign treatment from the estimates
of   the  blip   function   depend   on  sample   size,   by  specifying   the
\Robject{Gexploit} and \Robject{Gexplore} arguments.

<<MOR:lasso, warning=FALSE, eval=TRUE>>=
## MOR, lasso example
Gexploit <- function(nn) {
  if (nn <= 300) {
    exploit <- 0.15
  } else if (nn <= 400) {
    exploit <- 0.10
  } else if (nn <= 500) {
    exploit <- 0.05
  } else {
    exploit <- 0.025
  }
  return(exploit)
}
Gexplore <- Gexploit

MOR.lasso <- tsml.cara.rct(what="MOR", flavor="lasso",
                           ninit=200, by=100, nmax=500,
                           tm.init=oneOne, tm.ref=oneOne,
                           learnQ=makeLearnQ, Gmin=0.05,
                           Gexploit=Gexploit, Gexplore=Gexplore,
                           conf.level=0.95,
                           piV=piV, family=family,
                           Qbar=Qbar2, Vbar=Vbar2)                           
MOR.lasso
@ 

A ``history''  of the procedure is  contained in \Rcode{MOR.lasso}. It  can be
retrieved by running:

<<MOR:history, eval=TRUE>>=
MOR.lasso.hist <- getHistory(MOR.lasso)$hist
MOR.lasso.hist
@ 

A visual representation of the course  of the procedure, including regrets and
their estimation, is shown in Figure~\ref{fig/fig:MOR:lasso-1}. It is obtained
by running:

<<fig:MOR:lasso, fig.show='hide', fig.width=6, fig.height=6, eval=TRUE>>=
plot(MOR.lasso, 
     truth=list(truth, attr(MOR.lasso, "truthn")),
     regret=list(attr(MOR.lasso, "eregretn"), attr(MOR.lasso, "cregretn")))
@

\incfig{fig/fig:MOR:lasso-1}{0.5\textwidth}{The   course   of  the   procedure
  summarized by \Robject{MOR.lasso}.}{}



It is  interesting to compare the  above procedure with the  similar procedure
which differs  from it  only by  the use of  the balanced  treatment mechanism
$\gb$    throughout.    Setting    \Rcode{Gmin=0.5}    in    the    call    to
\Rfunction{tsml.cara.rct} imposes the use of $\gb$.

<<MOR.balanced, warning=FALSE, eval=TRUE>>=
MOR.balanced <- tsml.cara.rct(what="MOR", flavor="lasso",
                              ninit=200, by=100, nmax=500,
                              tm.init=oneOne, tm.ref=oneOne,
                              learnQ=makeLearnQ, Gmin=0.5,
                              conf.level=0.95,
                              piV=piV, family=family,
                              Qbar=Qbar2, Vbar=Vbar2)                           
MOR.balanced
MOR.balanced.hist <- getHistory(MOR.balanced)$hist

## comparing variances and regrets
idx <- grep("target", rownames(MOR.balanced.hist))
nobs <- MOR.balanced.hist[idx, "nobs"]
var.ratio <- (MOR.balanced.hist[idx, "psi.sd"]/MOR.lasso.hist[idx, "psi.sd"])^2
regret.ratio <- MOR.balanced.hist[idx, "regret"]/MOR.lasso.hist[idx, "regret"]
cbind(nobs=nobs, var.ratio=var.ratio, regret.ratio=regret.ratio)
@ 


\subsubsection{Step by step simulation}

It is also possible to carry out the procedure step by step. 

<<MOR:lasso:step, warning=FALSE, eval=TRUE>>=
## MOR, another lasso example, step by step
## ## sample
obs <- getSample(200, 
                 piV=piV, family=family,
                 Qbar=Qbar2, Vbar=Vbar2)
nobs <- nrow(obs)
## ## initialize
MOR.lasso.step <- tsml.cara.rct:::TSMLCARA(obs=obs,
                                           what="MOR", flavor="parametric",
                                           learnQ=makeLearnQ, Gmin=0.1, verbose=log)
## ## update, target
tsml.cara.rct:::update.TSMLCARA(MOR.lasso.step, verbose=log)
tsml.cara.rct:::targetPsi.TSMLCARA(MOR.lasso.step, verbose=log)
## ## estimating the true values of the data-adaptive parameters
Qn <- tsml.cara.rct:::getQ.TSMLCARA(MOR.lasso.step)
ruleQn <- tsml.cara.rct:::ruleQbar(Qn)
## ## ## mean of current estimated treatment rule
Bn <- 1e4 ## one may wish to use a larger integer
truthn <- getSample(Bn, 
                    tm=oneOne, rule=ruleQn,
                    piV=piV, family=family, 
                    Qbar=Qbar2, Vbar=Vbar2, what="MOR")[c("psi", "psi.sd")] 
## ## ## empirical cumulated regret
W <- tsml.cara.rct:::extractW(obs)
rA <- as.numeric(ruleQn(W))
eregretn <- mean(obs[, "Y"] - Qbar2(cbind(A=rA, W)))
## ## ## counterfactual cumulated regret
col <- match(c("Y0", "Y1"), colnames(obs))
rA <- col[rA+1]
counterfactual <- as.numeric(obs[cbind(1:nrow(obs), rA)])
cregretn <- mean(obs[, "Y"] - counterfactual)
while(nobs<500) {
  gn <- tsml.cara.rct:::getGstar.TSMLCARA(MOR.lasso.step)
  ## ## sample
  newObs <- getSample(100, tm=gn,
                      piV=piV, family=family,
                      Qbar=Qbar2, Vbar=Vbar2)
  tsml.cara.rct:::addNewSample.TSMLCARA(MOR.lasso.step, newObs)
  nobs <- nrow(getObs(MOR.lasso.step))
  ## ## update, target
  tsml.cara.rct:::update.TSMLCARA(MOR.lasso.step, verbose=log)
  tsml.cara.rct:::targetPsi.TSMLCARA(MOR.lasso.step, verbose=log)
  ## ## estimating the true values of the data-adaptive parameters
  Qn <- tsml.cara.rct:::getQ.TSMLCARA(MOR.lasso.step)
  ruleQn <- tsml.cara.rct:::ruleQbar(Qn)
  ## ## ## mean of current estimated treatment rule
  truthn <- rbind(truthn,
                  getSample(Bn,
                            tm=oneOne, rule=ruleQn,
                            piV=piV, family=family, 
                            Qbar=Qbar2, Vbar=Vbar2, what="MOR")[c("psi", "psi.sd")])
  ## ## ## empirical cumulated regret
  obs <- getObs(MOR.lasso.step)
  W <- tsml.cara.rct:::extractW(obs)
  rA <- as.numeric(ruleQn(W))
  eregretn <- cbind(eregretn,
                    mean(obs[, "Y"] - Qbar2(cbind(A=rA, W))))
  ## ## ## counterfactual cumulated regret
  col <- match(c("Y0", "Y1"), colnames(obs))
  rA <- col[rA+1]
  counterfactual <- as.numeric(obs[cbind(1:nrow(obs), rA)])
  cregretn <- rbind(cregretn,
                    mean(obs[, "Y"] - counterfactual))

}
truthn[, "psi.sd"] <- truthn[, "psi.sd"]/sqrt(Bn)
colnames(truthn) <- c("psin", "psin.sd")
rownames(truthn) <- NULL
attr(MOR.lasso.step, "truthn") <- truthn
attr(MOR.lasso.step, "eregretn") <- eregretn
attr(MOR.lasso.step, "cregretn") <- cregretn
MOR.lasso.step
@ 


\section{Extensions}

The       functions       \Rfunction{getSample},       \Rfunction{makeLearnQ},
\Rfunction{makeLearnQ.piecewise} and more generally all exported functions can
serve as templates to extend the functionalities of the package.

\section{Session information}
\label{sec:session-information}

Here is  the output  of \Rfunction{sessionInfo}  on the  system on  which this
document was compiled. 

<<sessionInfo>>=
sessionInfo()
@


\begin{thebibliography}{}
\bibitem[Chambaz et~al., 2015]{chambaz15} 
  Chambaz,  A., van~der Laan, M.~J.,  and    Zheng,   W.    (2015).     
  \newblock   Targeted    covariate-adjusted   response-adaptive   LASSO-based
  randomized controlled trials.
  \newblock Book chapter  in {\em Modern Adaptive  Randomized Clinical Trials:
    Statistical, Operational, and Regulatory Aspects} by O. Sverdlov.
  \newblock Chapman and Hall/CRC.

\bibitem[Zheng et~al., 2016]{zheng16} 
  Zheng, W., Chambaz, A., and van~der Laan, M.~J. (2016).  
  \newblock   Drawing  valid   targeted   inference  when   covariate-adjusted
  response-adaptive  RCT meets  data-adaptive loss-based  estimation, with  an
  application to the LASSO. 
  \newblock   U.C.   Berkeley   Division  of   Biostatistics   Working   Paper
  Series. Working paper 339.
  \newblock {\em Submitted}. http://biostats.bepress.com/ucbbiostat/paper339.
  
\bibitem[Chambaz et~al., 2016]{chambaz16}
  Chambaz, A., Zheng, W., and van~der Laan, M.~J. (2016).
  \newblock Data-adaptive inference of the optimal treatment rule and its mean reward. The masked bandit.
  \newblock   U.C.   Berkeley   Division  of   Biostatistics   Working   Paper
  Series. Working paper 349.
  \newblock {\em Submitted}. http://biostats.bepress.com/ucbbiostat/paper349.
\end{thebibliography}



\end{document}




